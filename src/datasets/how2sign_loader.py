# src/datasets/how2sign_dataset.py

import json
from pathlib import Path
from typing import Any, Dict, List, Optional

import torch
from torch.utils.data import Dataset

from src.utils.video_io import extract_frames_from_video


class How2SignDataset(Dataset):
    """
    Clip-level dataset for the How2Sign dataset.

    This dataset reads a pre-built JSON file (how2sign_dataset.json) and:
      - resolves the absolute path of each video
      - optionally extracts video frames on-the-fly
      - returns visual inputs (frames or video paths), target text, and metadata

    The JSON file is generated by build_how2sign_json.py and contains
    split information and clip-level annotations.
    """

    def __init__(
        self,
        json_path: str = "data/How2Sign_resized/how2sign_dataset.json",
        split: str = "train",                       # "train" | "val" | "test"
        n_frames_to_take: Optional[int] = 32,       # None = use all frames
        frame_sampling_strategy: str = "uniform",   # "uniform" | "consecutive" | "fps2_max32"
        root_dir: Optional[str] = None,             # Optional override for the dataset root directory
        return_type: str = "video",                 # "images" | "video"
    ) -> None:
        super().__init__()                          # Call the constructor of the parent class Dataset (for __getitem__ ecc ecc)

        # modalitÃ  di ritorno del dataset: "images" (frame estratti) o "video" (solo path al file video)
        self.return_type = return_type
        if self.return_type not in ("images", "video"):
            raise ValueError(
                f"return_type deve essere 'images' oppure 'video', trovato: {self.return_type}"
            )

        self.json_path = Path(json_path).resolve()
        if not self.json_path.exists():
            raise FileNotFoundError(f"JSON non trovato: {self.json_path}")

        with self.json_path.open("r", encoding="utf-8") as f: # open the json in read-mode
            data = json.load(f)                               # the json becomes a dict

        self.meta: Dict[str, Any] = data.get("meta", {})                        # save metadata (meta field of the json)
        self.splits: Dict[str, List[Dict[str, Any]]] = data.get("splits", {})   # save dataset entries (splits meta of the json)

        if split not in self.splits:
            raise ValueError(f"Split '{split}' non presente nel JSON. Split disponibili: {list(self.splits.keys())}")

        self.split = split                                          
        self.entries: List[Dict[str, Any]] = self.splits[split] # save the entries of the selected split

        # entries is my list of dict where each dict is a clip with fields like:
        # {
        #   "video_id": "xyz",
        #   "video_name": "xyz.mp4",
        #   "video_path": "data/How2Sign/train_raw_videos/xyz.mp4",
        #   "sentence_id": "abc",
        #   "sentence_name": "abc.txt",
        #   "sentence": "This is the ground truth sentence.",
        #   "start_realigned": 12.34,
        #   "end_realigned": 15.67,
        #   "split": "train"
        # }

        self.n_frames_to_take = n_frames_to_take
        self.frame_sampling_strategy = frame_sampling_strategy
        
        # Determine the root directory for videos (override with root_dir if provided)
        meta_root = self.meta.get("root_dir")
        if root_dir is not None:
            self.root_dir = Path(root_dir).resolve()
        elif meta_root is not None:
            self.root_dir = Path(meta_root).resolve()
        else:
            self.root_dir = self.json_path.parents[2]

        print(
            f"[How2SignDataset] split={self.split} | num_samples={len(self.entries)} | "
            f"n_frames_to_take={self.n_frames_to_take} | strategy={self.frame_sampling_strategy} | "
            f"return_type={self.return_type}"
        )
        print(f"[How2SignDataset] json_path={self.json_path}")
        print(f"[How2SignDataset] root_dir={self.root_dir}")

    def __len__(self) -> int:
        return len(self.entries) # return the number of samples in the selected split of the dataset

    def __getitem__(self, idx: int) -> Dict[str, Any]: # return the dict of a sample given its index
        entry = self.entries[idx]

        # realtive path of the video (es. "data/How2Sign/train_raw_videos/....mp4")
        video_rel = Path(entry["video_path"])
        video_abs = (self.root_dir / video_rel).resolve()

        if not video_abs.exists():
            raise FileNotFoundError(f"Video non trovato: {video_abs}")

        # target text (ground truth)
        sentence: str = entry["sentence"]

        if self.return_type == "images":
            # Extract frames on-the-fly
            frames = extract_frames_from_video(
                video_path=str(video_abs),
                n_frames_to_take=self.n_frames_to_take,
                strategy=self.frame_sampling_strategy,
            )

            sample: Dict[str, Any] = {
                "images": frames,                 # List[PIL.Image]
                "target_text": sentence,          # stringa inglese ground truth
                "split": entry["split"],
                "video_path": str(video_abs),
                "video_rel_path": entry["video_path"],
                "video_id": entry["video_id"],
                "video_name": entry["video_name"],
                "sentence_id": entry["sentence_id"],
                "sentence_name": entry["sentence_name"],
                "start_realigned": entry["start_realigned"],
                "end_realigned": entry["end_realigned"],
            }
        else:
            # in video-mode: we don't extract frames, we let the processor (e.g., Qwen3VLVideoProcessor)
            # handle the video from the path.
            sample: Dict[str, Any] = {
                # no "images" field here: the collate_fn will handle this mode by returning "videos"
                "target_text": sentence,          # stringa inglese ground truth
                "split": entry["split"],
                "video_path": str(video_abs),
                "video_rel_path": entry["video_path"],
                "video_id": entry["video_id"],
                "video_name": entry["video_name"],
                "sentence_id": entry["sentence_id"],
                "sentence_name": entry["sentence_name"],
                "start_realigned": entry["start_realigned"],
                "end_realigned": entry["end_realigned"],
            }

        return sample


def how2sign_collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Custom collate function for the How2Sign dataset.

    This function is called by PyTorch's DataLoader at every iteration to
    transform a list of individual samples (returned by Dataset.__getitem__)
    into a single batch.

    Responsibilities of this collate function:
      - Group visual inputs (either frame lists or video paths)
      - Group target texts
      - Keep metadata as a list of dictionaries (one per sample)
    
    This collate function doesn't convert data to torch.Tensors since we want to use qwen processor for that

    The actual batching into tensors is deferred to the training script,
    where a multimodal processor (e.g., Qwen3-VL processor) is available.
    """

    # We inspect the first sample to determine which modality is being used.
    if "images" in batch[0]:
        
        images_batch = [sample["images"] for sample in batch ]  # List[List[PIL.Image]]
        texts_batch = [sample["target_text"] for sample in batch ]  # List[str]

        # Collect all remaining fields as metadata.
        # Metadata is kept separate and untouched, useful for debugging,
        meta_batch: List[Dict[str, Any]] = []
        for sample in batch:
            meta = {
                k: v
                for k, v in sample.items()
                # Exclude fields that are already explicitly batched
                if k not in ("images", "target_text")
            }
            meta_batch.append(meta)

        # Return a structured batch dictionary.
        # This output will be consumed by the training loop.
        return {
            "images": images_batch,   # List[List[PIL.Image]]
            "texts": texts_batch,     # List[str]
            "meta": meta_batch,       # List[Dict[str, Any]]
        }

    
    videos_batch = [sample["video_path"] for sample in batch] # List[str] (absolute video paths)
    texts_batch = [sample["target_text"] for sample in batch] # List[str]

    meta_batch: List[Dict[str, Any]] = []
    for sample in batch:
        meta = {
            k: v
            for k, v in sample.items()
            # "images" is not expected here, but we keep the same exclusion logic
            # to maintain symmetry between modalities.
            if k not in ("images", "target_text")
        }
        meta_batch.append(meta)

    return {
        "videos": videos_batch,   # List[str] (absolute video paths)
        "texts": texts_batch,     # List[str]
        "meta": meta_batch,       # List[Dict[str, Any]]
    }